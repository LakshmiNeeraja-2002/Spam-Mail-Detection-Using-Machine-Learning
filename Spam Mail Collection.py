# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aUFdqHiHYuocfwaks1JabIuxtcnJkvvx
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer
nltk.download("wordnet")
nltk.download("omw-1.4")

import spacy



from wordcloud import WordCloud
from collections import Counter
import string

import matplotlib.pyplot as plt
plt.style.use('ggplot')
import seaborn as sns
sns.set_style('whitegrid')

df=pd.read_csv('spam.csv',encoding='latin-1')

df.head()

df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)

df.head()

df.rename(columns={'v1':'target','v2':'message'},inplace=True)
df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df

#Encoding
encoder=LabelEncoder()
df['target']=encoder.fit_transform(df['target'])
df

#EDA
df['target'].value_counts().plot(kind='bar')

plt.pie(df['target'].value_counts(),labels=['ham','spam'],autopct='%0.2f%%')
plt.show()

#featuring Engineering
# number of characters in messages
df['characters']=df['message'].apply(len)
# number of words in messages
df['words']=df['message'].apply(lambda x:len(nltk.word_tokenize(x)))
# number of sentences in messages
df['sentences']=df['message'].apply(lambda x:len(nltk.sent_tokenize(x)))
df

df[["characters","words","sentences"]].describe()

df[df['target']==0][["characters","words","sentences"]].describe()

df[df['target']==1][["characters","words","sentences"]].describe()

sns.histplot(df[df['target']==0]['characters'],color='green')
sns.histplot(df[df['target']==1]['characters'],color='red')

sns.histplot(df[df['target']==0]['sentences'],color='green')
sns.histplot(df[df['target']==1]['sentences'],color='red')

sns.histplot(df[df['target']==0]['words'],color='green')
sns.histplot(df[df['target']==1]['words'],color='red')

sns.pairplot(df,hue='target')

sns.heatmap(df[['target','words','sentences','characters']].corr())

#Preprocessing
!pip install nltk==3.2.4

nlp = spacy.load('en_core_web_sm')

def lemmatize(word):
    doc=nlp(word)
    for token in doc:
        return token.lemma_

def preprocess(text):
    import string


    text=text.lower()
    text=nltk.word_tokenize(text)
    text=[lemmatize(word) for word in text if word not in nltk.corpus.stopwords.words('english')]
    text=[word for word in text if word not in string.punctuation]
    text=[word for word in text if word.isalnum()]
    text=' '.join(text)
    return text

df['transformed']=df['message'].apply(preprocess)
df

wordcloud=WordCloud(width=1800,height=1200,background_color='white',min_font_size=10)
plt.imshow(wordcloud.generate(df[df['target']==0]['message'].to_string()))

wordcloud=WordCloud(width=1800,height=1200,background_color='white',min_font_size=10)
plt.imshow(wordcloud.generate(df[df['target']==1]['message'].to_string()))

spam_words=[]
for i in df[df['target']==1]['transformed'].to_list():
    for word in i.split():
        spam_words.append(word)
pd.DataFrame(Counter(spam_words).most_common(30)).plot(kind='bar',x=0,y=1,color='red')

ham_words=[]
for i in df[df['target']==0]['transformed'].to_list():
    for word in i.split():
        ham_words.append(word)
pd.DataFrame(Counter(ham_words).most_common(30)).plot(kind='bar',x=0,y=1,color='green')

df.to_csv('spam1.csv',index=False)

from sklearn.feature_extraction.text import TfidfVectorizer
cv=CountVectorizer()
#tfidf=TfidfVectorizer(encoding="latin-1",decode_error="ignore")
X=cv.fit_transform(df['transformed']).toarray()
#df['transformed'].fillna(" ", inplace=True)
X=cv.fit_transform(df['transformed']).toarray()
y=df['target'].values
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import MultinomialNB
mnb=MultinomialNB()
from sklearn.naive_bayes import GaussianNB
gnb=GaussianNB()
from sklearn.naive_bayes import BernoulliNB
bnb=BernoulliNB()
from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier()
from sklearn.svm import SVC
svc=SVC()
from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
from sklearn.tree import DecisionTreeClassifier
dtc=DecisionTreeClassifier()
from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
from sklearn.ensemble import AdaBoostClassifier
abc=AdaBoostClassifier()
from sklearn.ensemble import GradientBoostingClassifier
gbc=GradientBoostingClassifier()
from sklearn.ensemble import ExtraTreesClassifier
etc=ExtraTreesClassifier()
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,precision_score

models=[mnb,gnb,bnb,rfc,svc,lr,dtc,knn,abc,gbc,etc]
model_names=['MultinomialNB','GaussianNB','BernoulliNB','RandomForestClassifier','SVC','LogisticRegression','DecisionTreeClassifier','KNeighborsClassifier','AdaBoostClassifier','GradientBoostingClassifier','ExtraTreesClassifier']
accuracy=[]
for model in range(len(models)):
    clf=models[model]
    clf.fit(X_train,y_train)
    y_pred=clf.predict(X_test)
    accuracy.append(accuracy_score(y_test,y_pred))
    print('Model name:  {}   Accuracy:  {}'.format(model_names[model],round(accuracy_score(y_test,y_pred),2)))
    print('Model name:  {}   Precision:  {}'.format(model_names[model],round(precision_score(y_test,y_pred),2)))

